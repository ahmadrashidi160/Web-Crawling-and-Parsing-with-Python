{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebc9b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: requests-html in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: parse in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (1.19.0)\n",
      "Requirement already satisfied: w3lib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (1.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (2.28.1)\n",
      "Requirement already satisfied: pyquery in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (1.1.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (1.0.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-html) (0.0.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.11)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (8.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.64.1)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2022.9.14)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.11.1)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fake-useragent->requests-html) (5.12.0)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (1.2.0)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->requests-html) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->requests-html) (3.3)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from w3lib->requests-html) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (2.3.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bleach) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bleach) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bleach) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->bleach) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "# Installing Libraries\n",
    "!pip install requests\n",
    "!pip install requests-html\n",
    "!pip install bleach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af310468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: C:\\Users\\Admin\\Desktop\\output.csv\n"
     ]
    }
   ],
   "source": [
    "# Importing Necesessary Libraries\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "from requests_html import HTMLSession\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Defining function to extract the text contents of HTML webpage\n",
    "def extract_text(url):\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    paragraphs = r.html.find('p')\n",
    "    text = ''\n",
    "    for paragraph in paragraphs:\n",
    "        text += paragraph.text + ' '\n",
    "    return text\n",
    "\n",
    "# Defining function to extract the links, text contents, and then perform part of speech tagging\n",
    "def crawl_website(domain):\n",
    "    session = HTMLSession()\n",
    "    r = session.get(domain)\n",
    "    links = r.html.absolute_links\n",
    "    data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            if domain in link:\n",
    "                text = extract_text(link)\n",
    "                blob = TextBlob(text)\n",
    "                tags = blob.tags\n",
    "                data.append((link, text, tags))\n",
    "        except:\n",
    "            continue \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Link\", \"Text\", \"Tags\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "domain = 'https://en.wikipedia.org/wiki/Harvard_University'\n",
    "data = crawl_website(domain)\n",
    "filename = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"output.csv\")\n",
    "save_to_csv(data, filename)\n",
    "print(\"Output saved to:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c25621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
